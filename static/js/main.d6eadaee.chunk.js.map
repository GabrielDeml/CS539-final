{"version":3,"sources":["logo.svg","youtube.js","images/download.jpeg","images/2021-12-15_14-01.png","images/2021-12-15_15-30.png","images/2021-12-15_15-30_1.png","images/2021-12-15_14-22_2.png","images/2021-12-15_14-22.png","images/2021-12-15_14-21_2.png","images/2021-12-15_15-38.png","images/2021-12-15_14-21_1.png","images/2021-12-15_14-21.png","images/2021-12-15_15-42.png","App.js","reportWebVitals.js","index.js"],"names":["YoutubeEmbed","embedId","className","width","height","src","frameBorder","allow","allowFullScreen","title","App","segmentation","alt","dataset_image","href","enet_image","enet_image_2","enet_image_3","enet_image_4","enet_image_5","enet_image_6","enet_image_7","enet_image_8","enet_image_9","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"oKAAe,I,WCqBAA,EAlBM,SAAC,GAAD,IAAGC,EAAH,EAAGA,QAAH,OACnB,qBAAKC,UAAU,UAAf,SACE,wBACEC,MAAM,MACNC,OAAO,MACPC,IAAG,wCAAmCJ,GACtCK,YAAY,IACZC,MAAM,2FACNC,iBAAe,EACfC,MAAM,wBCZG,MAA0B,sCCA1B,MAA0B,6CCA1B,MAA0B,6CCA1B,MAA0B,+CCA1B,MAA0B,+CCA1B,MAA0B,6CCA1B,MAA0B,+CCA1B,MAA0B,6CCA1B,MAA0B,+CCA1B,MAA0B,6CCA1B,MAA0B,6CC8G1BC,MA/Ff,WACE,OAEE,qBAAKR,UAAU,MAAf,SAEE,yBAAQA,UAAU,aAAlB,UACE,yDACA,2CACA,2CACA,8CACA,4HACA,qDACA,gEACA,sfACA,qBAAKG,IAAKM,EAAcC,IAAI,uBAAuBV,UAAU,iBAC7D,4BAAYA,UAAU,gBAAtB,+GACA,8FACA,yaACA,+CACA,ufACA,+BACE,oGACA,kHAEF,4CACA,+WACA,yCACA,yNACA,idACA,qBAAKG,IAAKQ,EAAeD,IAAI,iBAAiBV,UAAU,iBACxD,4BAAYA,UAAU,gBAAtB,qFACA,0DACA,iTAAmR,mBAAGY,KAAK,uCAAR,kBAAnR,4KACA,qBAAKT,IAAKU,EAAYH,IAAI,qBAAqBV,UAAU,iBACzD,4BAAYA,UAAU,gBAAtB,+CACA,4BAAG,yBACH,qBAAKG,IAAKW,EAAcJ,IAAI,qBAAqBV,UAAU,iBAC3D,4BAAYA,UAAU,gBAAtB,uEACA,4BAAG,yBACH,qBAAKG,IAAKY,EAAcL,IAAI,qBAAqBV,UAAU,iBAC3D,4BAAYA,UAAU,gBAAtB,6DACA,6CACA,sDACA,2QACA,qBAAKG,IAAKa,EAAcN,IAAI,qBAAqBV,UAAU,iBAC3D,4BAAYA,UAAU,gBAAtB,mHACA,uDACA,wVACA,qBAAKG,IAAKc,EAAcP,IAAI,qBAAqBV,UAAU,iBAC3D,4BAAYA,UAAU,gBAAtB,oGACA,yCACA,iMACA,0DACA,miBACA,qBAAKG,IAAKe,EAAcR,IAAI,qBAAqBV,UAAU,iBAC3D,4BAAYA,UAAU,gBAAtB,0DACA,4BAAG,yBACH,qBAAKG,IAAKgB,EAAcT,IAAI,qBAAqBV,UAAU,iBAC3D,4BAAYA,UAAU,gBAAtB,6EACA,4BAAG,yBACH,qBAAKG,IAAKiB,EAAcV,IAAI,qBAAqBV,UAAU,iBAC3D,4BAAYA,UAAU,gBAAtB,gGACA,4BAAG,yBACH,qBAAKG,IAAKkB,EAAcX,IAAI,qBAAqBV,UAAU,iBAC3D,4BAAYA,UAAU,gBAAtB,yGACA,mSACA,6DACA,yEACA,cAAC,EAAD,CAAcD,QAAQ,gBACtB,8DACA,cAAC,EAAD,CAAcA,QAAQ,gBACtB,6CACA,wCACA,ocACA,scACA,uNACA,wCACA,+BACE,oPACA,mQACA,oQACA,iOAEF,mDACA,+BACE,sDAAwB,mBAAGa,KAAK,2CAAR,yDACxB,sDAAwB,mBAAGA,KAAK,8CAAR,6DACxB,0DAA4B,mBAAGA,KAAK,kEAAR,yFC1FvBU,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,6BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1Bb,K","file":"static/js/main.d6eadaee.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/logo.6ce24c58.svg\";","import React from \"react\";\nimport PropTypes from \"prop-types\";\n\nconst YoutubeEmbed = ({ embedId }) => (\n  <div className=\"picture\">\n    <iframe\n      width=\"853\"\n      height=\"480\"\n      src={`https://www.youtube.com/embed/${embedId}`}\n      frameBorder=\"0\"\n      allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n      allowFullScreen\n      title=\"Embedded youtube\"\n    />\n  </div>\n);\n\nYoutubeEmbed.propTypes = {\n  embedId: PropTypes.string.isRequired\n};\n\nexport default YoutubeEmbed;","export default __webpack_public_path__ + \"static/media/download.71d89593.jpeg\";","export default __webpack_public_path__ + \"static/media/2021-12-15_14-01.c0261617.png\";","export default __webpack_public_path__ + \"static/media/2021-12-15_15-30.77b25105.png\";","export default __webpack_public_path__ + \"static/media/2021-12-15_15-30_1.c1e7806f.png\";","export default __webpack_public_path__ + \"static/media/2021-12-15_14-22_2.d21ffeed.png\";","export default __webpack_public_path__ + \"static/media/2021-12-15_14-22.ef877179.png\";","export default __webpack_public_path__ + \"static/media/2021-12-15_14-21_2.37703ca9.png\";","export default __webpack_public_path__ + \"static/media/2021-12-15_15-38.4c8e80ab.png\";","export default __webpack_public_path__ + \"static/media/2021-12-15_14-21_1.c1b5d1eb.png\";","export default __webpack_public_path__ + \"static/media/2021-12-15_14-21.066dc286.png\";","export default __webpack_public_path__ + \"static/media/2021-12-15_15-42.a63ce5a8.png\";","import logo from './logo.svg';\nimport './App.css';\nimport YoutubeEmbed from \"./youtube.js\";\nimport segmentation from \"./images/download.jpeg\";\nimport dataset_image from \"./images/2021-12-15_14-01.png\";\nimport enet_image from \"./images/2021-12-15_15-30.png\";\nimport enet_image_2 from \"./images/2021-12-15_15-30_1.png\";\nimport enet_image_3 from \"./images/2021-12-15_14-22_2.png\";\nimport enet_image_4 from \"./images/2021-12-15_14-22.png\";\nimport enet_image_5 from \"./images/2021-12-15_14-21_2.png\";\nimport enet_image_6 from \"./images/2021-12-15_15-38.png\";\nimport enet_image_7 from \"./images/2021-12-15_14-21_1.png\";\nimport enet_image_8 from \"./images/2021-12-15_14-21.png\";\nimport enet_image_9 from \"./images/2021-12-15_15-42.png\";\n\nfunction App() {\n  return (\n\n    <div className=\"App\">\n\n      <header className=\"App-header\">\n        <h1>CS 539 Machine Learning</h1>\n        <h3>Professor</h3>\n        <p>Kyumin Lee</p>\n        <h3>Team Members</h3>\n        <p>Khizar Mohammed Amjed Mohamed, Gabriel Deml, Ghokulji Selvaraj, Samarth Shah, Akhil Daphara</p>\n        <h3>Project Description</h3>\n        <h4>What is Semantic segmentation?</h4>\n        <p>A picture consists of objects in it, like trees, cars, people, etc. These objects are called classes. Each of these classes is made up of a set of pixels. Semantic segmentation is a technique where we label each pixel according to the class it belongs to. However, it doesnâ€™t differentiate between different instances of the same object. For example, if there are two cats in a picture. Semantic segmentation would not be able to assign each cat a different label.</p>\n        <img src={segmentation} alt=\"Segmentation Example\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure 1. Picture was taken from http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf</figcaption>\n        <h4>Why use semantic segmentation in Autonomous driving domains?</h4>\n        <p>Autonomous vehicles are real-time domains where decisions need to be made in the order of milliseconds. Semantic segmentation takes videos captured in a self-driving environment and annotates every pixel of every frame in a video. These segmented videos are then used to calculate the driveable areas on an image.  It is therefore crucial that semantic segmentation is both fast and accurate.</p>\n        <h3>Project Goals</h3>\n        <p>With our initial literature survey, we came upon the E-net Neural Network architecture, used for real-time semantic segmentation. The authors of this architecture claimed that it could semantically segment videos at a rate of  135.4 frames per second on a Titan X GPU. However, it lacked accuracy. Therefore we decided to make changes to the architecture to improve the accuracy without a considerable loss in speed. In summary, these were the objectives of our project.</p>\n        <ol>\n          <li>Train and test the E-Net and Segnet Convolutional Neural Networks.</li>\n          <li>Suggest possible improvements in the E-Net model to increase its performance.</li>\n        </ol>\n        <h3>Tools Used</h3>\n        <p>E-Net is a relatively small Neural Network. It has around 0.37M parameters. Because of the small size of the architecture, we were able to train it on our Laptops. We used an Nvidia GeForce GTX 1050 (4GB GDDR5) GPU and Intel Core i7-7700HQ 2.8-3.8GHz CPU. However, we also intermittently used Google Co-Lab to test out design changes.</p>\n        <h4>Dataset</h4>\n        <p>The CItyscape dataset was used as it contains high-quality pictures of urban and road scenes. Since we are focusing on the autonomous vehicles domain, this dataset suited our purposes.</p>\n        <p>Two versions of the dataset were used for the project. The full dataset contained all the fine image data available on CItyscapes. However, since using the entire dataset would take a considerable amount of time, a reduced dataset set was created which was roughly half the size. This reduced dataset was primarily used for testing small design changes. The details of both versions of the dataset are provided in the figure below. </p>\n        <img src={dataset_image} alt=\"Dataset Images\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure2: Versions of the dataset used include the half and full dataset</figcaption>\n        <h3>Basic E-Net architecture</h3>\n        <p>The E-Net architecture contains a convolutional neural network in both the encoder and decoder stages. It is also heavily inspired by ResNet architecture as it uses bottleneck modules in the encoder and decoder stages. Further explanation of the architecture can be found <a href=\"https://arxiv.org/pdf/1606.02147.pdf\">here</a>. We ran the base E-Net architecture with the parameters mentioned in Figure 3 on the above-mentioned hardware and were able to achieve the results shown in Figure 5.</p>\n        <img src={enet_image} alt=\"E-Net Architecture\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure 3: Base E-Net architecture</figcaption>\n        <p><br></br></p>\n        <img src={enet_image_2} alt=\"E-Net Architecture\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure4: Visual Representation of the E-Net architecture.</figcaption>\n        <p><br></br></p>\n        <img src={enet_image_3} alt=\"E-Net Architecture\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure5: Results of the Base-Enet Architecture.</figcaption>\n        <h3>Experiments</h3>\n        <h4>Activation functions</h4>\n        <p>Activation functions are applied between every convolution, hence they play an important role in deciding how we learn our weights.  In our experiments, we decided to change the activation functions to see how it affects our accuracy.</p>\n        <img src={enet_image_4} alt=\"E-Net Architecture\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure 6: Results comparing the change in mean IOU with the change in the encoder-decoder activation.</figcaption>\n        <h4>Increased Bottlenecks</h4>\n        <p>The E-Net architecture has an unbalanced encoder-decoder structure. The authors built this on the intuition that the decoder is only meant to fine-tune the results. However, we decided to test this hypothesis and increased the bottlenecks in the decoder. The results of our experiments can be found in figure 7.</p>\n        <img src={enet_image_5} alt=\"E-Net Architecture\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure 7: Effect of increased bottlenecks on the mean IOU compared with the Base E-net</figcaption>\n        <h3>Results</h3>\n        <p>Out of all the experiments we conducted, trying the Mish activation function on the entire dataset yielded the best results. The reason for this is given below.</p>\n        <h4>Mish Activation Function</h4>\n        <p>The bottleneck modules in the encoder have negative weights. The E-Net uses PReLU as the activation function between each convolution. Mish activation performed well because of its smoothness and non-monotonic properties. The intuition behind using Mish is that the smoothness helps in easier optimization and generalization. The non-monotonic property of Mish helps preserve the small negative weights, which helps in capturing patterns underlying the data. Figure 8 compares Mish with other activation functions.</p>\n        <img src={enet_image_6} alt=\"E-Net Architecture\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure 8: Comparison of activation functions</figcaption>\n        <p><br></br></p>\n        <img src={enet_image_7} alt=\"E-Net Architecture\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure 9: Results of the Modified E-net, Mean IoU and Class IoU</figcaption>\n        <p><br></br></p>\n        <img src={enet_image_8} alt=\"E-Net Architecture\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure 10: summary of the comparison between the modified E-Net and the base E-net</figcaption>\n        <p><br></br></p>\n        <img src={enet_image_9} alt=\"E-Net Architecture\" className=\"image_center\" />\n        <figcaption className='image_caption'>Figure 11: Comparison of the change in the Class IoU between Modified E-Net and base E-Net.</figcaption>\n        <p>From figure 11, it is observed that there is an increase in the Class IoU of smaller objects ( Pole, Fence, Rider, Traffic Light). Mish preserves the small negative weights which might be the reason we are seeing an increased IOU in the small object classes.</p>\n        <h3>Example output of the model</h3>\n        <h4>Modified Model on the Cityscape dataset</h4>\n        <YoutubeEmbed embedId=\"Yd0H_Sd_PYo\" />\n        <h4>Modified Model on WPI campus</h4>\n        <YoutubeEmbed embedId=\"yfLXqceukQU\" />\n        <h3>Discussions</h3>\n        <h4>SegNet</h4>\n        <p>SegNet takes a long time to train. It would have taken over a week to train to 200 epocs on the CityScapes dataset using our computers. Because of the time to train we were unable to train SegNet to a reasonable accuracy. Taking so much compute power to train the model may not be a problem for large autonomous car producers, it is a problem for smaller groups. Enet is the clear winner in the category of performance.</p>\n        <p>Our intent at first was to write SegNet ourselves based on the paper and compare it to Enet. We were able to make significant progress on this over a couple of weeks. Given a few more weeks we would have been able to finish. We kept having problems with our model and decided to train a model already written. This is where we realized that even if we wrote our own model that it would not be possible for us to train it.</p>\n        <p>We are not out of luck with using SegNet as a baseline. The Enet paper compares Enet with SegNet on the cityscapes dataset. This is what we can compare our improvements to Enet with.</p>\n        <h3>Papers</h3>\n        <ol>\n          <li>Yu, H., Yang, Z., Tan, L., Wang, Y., Sun, W., Sun, M., & Tang, Y. (2018). Methods and datasets on semantic segmentation: A review. Neurocomputing, 304, 82â€“103. https://doi.org/10.1016/j.neucom.2018.03.037.</li>\n          <li>Paszke, A., Chaurasia, A., Kim, S., & Culurciello, E. (2016, June 7). Enet: A deep neural network architecture for real-time semantic segmentation. arXiv.org. Retrieved October 27, 2021, from https://arxiv.org/abs/1606.02147.</li>\n          <li>Badrinarayanan, V., Kendall, A., & Cipolla, R. (2016, October 10). SegNet: A deep convolutional encoder-decoder architecture for image segmentation. arXiv.org. Retrieved October 27, 2021, from https://arxiv.org/abs/1511.00561.</li>\n          <li>Long, J., Shelhamer, E., & Darrell, T. (2015, March 8). Fully convolutional networks for semantic segmentation. arXiv.org. Retrieved October 27, 2021, from https://arxiv.org/abs/1411.4038.</li>\n        </ol>\n        <h3>GitHub repos used</h3>\n        <ol>\n          <li>Enet original code: <a href=\"https://github.com/davidtvs/PyTorch-ENet\">https://github.com/davidtvs/PyTorch-ENet</a></li>\n          <li>Enet modified code: <a href=\"https://github.com/GabrielDeml/PyTorch-ENet\">https://github.com/GabrielDeml/PyTorch-ENet </a></li>\n          <li>SegNet original source: <a href=\"https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks\">https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks</a></li>\n        </ol>\n\n      </header>\n    </div>\n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}