{"version":3,"sources":["logo.svg","youtube.js","App.js","reportWebVitals.js","index.js"],"names":["YoutubeEmbed","embedId","className","width","height","src","frameBorder","allow","allowFullScreen","title","App","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"oKAAe,I,WCqBAA,EAlBM,SAAC,GAAD,IAAGC,EAAH,EAAGA,QAAH,OACnB,qBAAKC,UAAU,mBAAf,SACE,wBACEC,MAAM,MACNC,OAAO,MACPC,IAAG,wCAAmCJ,GACtCK,YAAY,IACZC,MAAM,2FACNC,iBAAe,EACfC,MAAM,wBC+CGC,MAvDf,WACE,OAEE,qBAAKR,UAAU,MAAf,SAEE,yBAAQA,UAAU,aAAlB,UACE,yDACA,2CACA,2CACA,8CAEA,cAAC,EAAD,CAAcD,QAAQ,gBACtB,4HACA,qDACA,2VACA,4CAEA,+BACE,6WACA,wOACA,iOAEF,2CACA,sLACA,+BACE,oIACA,kHAEF,4CACA,4cACA,gEACA,6TAKE,2DACA,mUAGF,2CACA,kZAEA,wCACA,+BACE,gPACA,+PACA,gQACA,mOCxCKU,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,6BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1Bb,K","file":"static/js/main.a0ac7c5d.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/logo.6ce24c58.svg\";","import React from \"react\";\nimport PropTypes from \"prop-types\";\n\nconst YoutubeEmbed = ({ embedId }) => (\n  <div className=\"video-responsive\">\n    <iframe\n      width=\"853\"\n      height=\"480\"\n      src={`https://www.youtube.com/embed/${embedId}`}\n      frameBorder=\"0\"\n      allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n      allowFullScreen\n      title=\"Embedded youtube\"\n    />\n  </div>\n);\n\nYoutubeEmbed.propTypes = {\n  embedId: PropTypes.string.isRequired\n};\n\nexport default YoutubeEmbed;","import logo from './logo.svg';\nimport './App.css';\nimport YoutubeEmbed from \"./youtube.js\";\n\nfunction App() {\n  return (\n\n    <div className=\"App\">\n\n      <header className=\"App-header\">\n        <h1>CS 539 Machine Learning</h1>\n        <h3>Professor</h3>\n        <p>Kyumin Lee</p>\n        <h3>Team Members</h3>\n        {/* Youtube video embed */}\n        <YoutubeEmbed embedId=\"Yd0H_Sd_PYo\" />\n        <p>Khizar Mohammed Amjed Mohamed, Gabriel Deml, Ghokulji Selvaraj, Samarth Shah, Akhil Daphara</p>\n        <h3>Project Description</h3>\n        <p>Autonomous vehicles are real-time domains where decisions need to be made in the order of milliseconds. Semantic segmentation takes videos captured in a self-driving environment and annotates every pixel of every frame in a video. These segmented videos are then used to calculate the driveable areas on an image. </p>\n        <h3>Challenges</h3>\n        {/* Numbered list */}\n        <ol>\n          <li>The current architecture such as ENet and SegNet are fast but have a low mean Intersection over Union (mIoU).Hence the challenge is to find different methods that would help us increase this criteria.Extensive Literature survey of various neural network architectures is required to draw intuition in order to bring about a change.</li>\n          <li>Deep learning techniques are often criticized to heavily depend on a large quantity of labeled data, and training a neural network on such a large dataset requires a lot of computational resources. </li>\n          <li>External conditions could also influence the performance of the model. A model could perform poorly with external factors including the lighting and shadows, weather conditions, and so on.</li>\n        </ol>\n        <h3>Objective</h3>\n        <p>Two popular architectures used for real-time semantic segmentation are ENet and SegNet. The main objectives that our team aims to accomplish include:</p>\n        <ol>\n          <li>Build the E-Net and SegNet model and compare its performance for performing Semantic Segmentation.</li>\n          <li>Suggest possible improvements in the E-Net model to increase its performance.</li>\n        </ol>\n        <h3>Evaluation</h3>\n        <p>Paszke et al2 have reported that the ENet architecture  is upto 18x faster, requires 75x less FLOPs, has 79x less parameters when compared to existing models.However the IoU of the ENet architecture is less when compared with other Neural Network architectures.To demonstrate the usefulness of our improved ENet architecture, we shall attempt to achieve a greater mean IoU.A description of IoU and FPS have been provided below.</p>\n        <h5>Intersection over union(IoU): </h5>\n        <p>\n          \n          The area of overlap between the ground truth and the prediction, divided by the area of union between the prediction and the ground truth is called Intersection over union.It is measured between 0–100% where 0 implies  no overlap and 1 implies perfectly overlapping segmentation.\n        </p>\n        \n          <h5>Frames per second (FPS): </h5>\n          <p>\n          The number of frames in a video that can be semantically segmented in a second is referred to as FPS. Since our semantic segmentation task is aimed towards autonomous driving, an FPS that is higher than the Image acquisition rate of the camera would provide real time semantic segmentation.\n        </p>\n        <h3>Resources</h3>\n        <p>The Cityscape dataset contains 5000 fine-annotated images with the ground truth. These images are captured from several cities in good/medium weather conditions and have 30 classes. Different road scenarios are captured which consist of many pedestrians and cyclists. This dataset was selected as it provides good diversity, complexity and volume, to our training data.</p>\n          \n        <h3>Papers</h3>\n        <ol>\n          <li>Yu, H., Yang, Z., Tan, L., Wang, Y., Sun, W., Sun, M., & Tang, Y.(2018).Methods and datasets on semantic segmentation: A review.Neurocomputing, 304, 82–103. https://doi.org/10.1016/j.neucom.2018.03.037</li>\n          <li>Paszke, A., Chaurasia, A., Kim, S., & Culurciello, E.(2016, June 7).Enet: A deep neural network architecture for real-time semantic segmentation.arXiv.org.Retrieved October 27, 2021, from https://arxiv.org/abs/1606.02147.</li>\n          <li>Badrinarayanan, V., Kendall, A., & Cipolla, R.(2016, October 10).SegNet: A deep convolutional encoder-decoder architecture for image segmentation.arXiv.org.Retrieved October 27, 2021, from https://arxiv.org/abs/1511.00561.</li>\n          <li>Long, J., Shelhamer, E., & Darrell, T.(2015, March 8).Fully convolutional networks for semantic segmentation.arXiv.org.Retrieved October 27, 2021, from https://arxiv.org/abs/1411.4038.</li>\n        </ol>\n      </header>\n    </div>\n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}